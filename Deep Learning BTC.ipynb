{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2db05e8",
   "metadata": {},
   "source": [
    "# Primeiros Blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ab1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "IN_CSV   = os.path.join(DATA_DIR, \"merged_btc_features.csv\")\n",
    "OUT_CSV  = os.path.join(DATA_DIR, \"merged_btc_features_CLEAN_real.csv\")\n",
    "\n",
    "df_raw = pd.read_csv(IN_CSV)\n",
    "print(\"RAW SHAPE:\", df_raw.shape)\n",
    "print(\"RAW COLUMNS:\", list(df_raw.columns)[:20], \"...\")\n",
    "\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "df_raw = df_raw.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"target\"] = (df[\"close\"].shift(-1) > df[\"close\"]).astype(\"Int64\")\n",
    "\n",
    "# Drop last row (no t+1 label)\n",
    "df = df.iloc[:-1, :].copy()\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---- 3) Drop non-features / metadata early ----\n",
    "# These were fully NaN in your export and should not be in feature_cols\n",
    "for col_to_drop in [\"symbol\", \"interval\"]:\n",
    "    if col_to_drop in df.columns:\n",
    "        df = df.drop(columns=[col_to_drop])\n",
    "\n",
    "# Optionally drop 'price_usd' to avoid duplicating 'close' signal\n",
    "if \"price_usd\" in df.columns:\n",
    "    drop_price_usd = True\n",
    "else:\n",
    "    drop_price_usd = False\n",
    "\n",
    "# ---- 4) Define feature set ----\n",
    "drop_cols = {\"date\", \"target\"}\n",
    "if drop_price_usd:\n",
    "    drop_cols.add(\"price_usd\")\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# ---- 5) Coerce numerics & clean missing ----\n",
    "for c in feature_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "before = len(df)\n",
    "df[feature_cols] = df[feature_cols].ffill()#.bfill()\n",
    "\n",
    "# Drop any rows still missing *after* ffill/bfill (usually early rolling windows)\n",
    "df = df.dropna(subset=feature_cols + [\"target\"]).reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "# ---- 6) Report & diagnostics ----\n",
    "print(f\"Cleaned rows: {before} → {after}\")\n",
    "if after > 0:\n",
    "    print(\"Date range:\", df[\"date\"].min().date(), \"→\", df[\"date\"].max().date())\n",
    "else:\n",
    "    print(\"Date range: None → None\")\n",
    "\n",
    "print(\"Feature count:\", len(feature_cols))\n",
    "\n",
    "# Helpful: show columns that still have NaNs (should be none after drop)\n",
    "na_left = df[feature_cols + [\"target\"]].isna().sum()\n",
    "na_left = na_left[na_left > 0].sort_values(ascending=False)\n",
    "if not na_left.empty:\n",
    "    print(\"\\nResidual NaNs (should be empty):\")\n",
    "    print(na_left)\n",
    "\n",
    "# If we somehow ended with zero rows, provide quick diagnostics\n",
    "if after == 0:\n",
    "    print(\"\\nDIAGNOSTICS: zero rows after cleaning.\")\n",
    "    na_counts = df_raw.isna().sum().sort_values(ascending=False).head(25)\n",
    "    print(\"- Top NaN counts in RAW:\")\n",
    "    print(na_counts)\n",
    "    print(\"\\nTop of RAW (5 rows):\")\n",
    "    print(df_raw.head(5))\n",
    "    print(\"\\nBottom of RAW (5 rows):\")\n",
    "    print(df_raw.tail(5))\n",
    "    raise RuntimeError(\"No usable rows after target/cleaning. Check diagnostics above.\")\n",
    "\n",
    "# ---- 7) Persist the clean intermediate ----\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nSaved CLEAN dataset → {OUT_CSV}\")\n",
    "print(\"Columns (first 30):\", df.columns[:30].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acbe30",
   "metadata": {},
   "source": [
    "# BLOCK 3B: Split 70/15/15 + StandardScaler + NPZ (tabular e sequências)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99100e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = \"data\"\n",
    "IN_CLEAN   = os.path.join(DATA_DIR, \"merged_btc_features_CLEAN.csv\")\n",
    "OUT_SPLIT  = os.path.join(DATA_DIR, \"split_boundaries.json\")\n",
    "OUT_SCALER = os.path.join(DATA_DIR, \"scaler.pkl\")\n",
    "OUT_TAB    = os.path.join(DATA_DIR, \"tabular_train_val_test.npz\")\n",
    "OUT_SEQ    = os.path.join(DATA_DIR, \"sequence_train_val_test.npz\")\n",
    "\n",
    "LOOKBACK = 30\n",
    "MIN_SEG  = LOOKBACK + 10  # segurança: cada split precisa pelo menos isso\n",
    "\n",
    "# ---------- 1) Load ----------\n",
    "df = pd.read_csv(IN_CLEAN, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "print(\"CLEAN SHAPE:\", df.shape)\n",
    "\n",
    "# ---------- 2) Escolha de features ----------\n",
    "drop_cols = {\"date\", \"target\", \"symbol\", \"interval\", \"price_usd\"} & set(df.columns)\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# garantir numérico\n",
    "for c in feature_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# checar target\n",
    "if \"target\" not in df.columns:\n",
    "    raise RuntimeError(\"Coluna 'target' não encontrada (deve vir do 3A).\")\n",
    "\n",
    "# eliminar linhas com NaN restantes (pós 3A já deve estar limpo)\n",
    "df = df.dropna(subset=feature_cols + [\"target\"]).reset_index(drop=True)\n",
    "N = len(df)\n",
    "\n",
    "# ---------- 3) Split 70/15/15 com mínimos ----------\n",
    "train_target = int(0.70 * N)\n",
    "val_target   = int(0.15 * N)\n",
    "test_target  = N - train_target - val_target\n",
    "\n",
    "def enforce_minimums(tt, vt, tst, min_seg, total_len):\n",
    "    tt = max(tt, min_seg)\n",
    "    rem = total_len - tt\n",
    "    vt = max(vt, min_seg)\n",
    "    if vt > rem - min_seg:\n",
    "        vt = max(min_seg, rem - min_seg)\n",
    "    tst = total_len - tt - vt\n",
    "    if tst < min_seg:\n",
    "        borrow = min(min_seg - tst, tt - min_seg)\n",
    "        tt -= borrow\n",
    "        tst += borrow\n",
    "        if tst < min_seg:\n",
    "            borrow2 = min(min_seg - tst, vt - min_seg)\n",
    "            vt -= borrow2\n",
    "            tst += borrow2\n",
    "    return tt, vt, tst\n",
    "\n",
    "train_len, val_len, test_len = enforce_minimums(train_target, val_target, test_target, MIN_SEG, N)\n",
    "\n",
    "train_end_idx = train_len - 1\n",
    "val_end_idx   = train_len + val_len - 1\n",
    "\n",
    "TRAIN_END = pd.Timestamp(df.loc[train_end_idx, \"date\"])\n",
    "VAL_END   = pd.Timestamp(df.loc[val_end_idx,   \"date\"])\n",
    "\n",
    "train_df = df[df[\"date\"] <= TRAIN_END].copy()\n",
    "val_df   = df[(df[\"date\"] > TRAIN_END) & (df[\"date\"] <= VAL_END)].copy()\n",
    "test_df  = df[df[\"date\"] > VAL_END].copy()\n",
    "\n",
    "print(\"=== SPLIT INFO (70/15/15) ===\")\n",
    "print(f\"Total rows: {N} | LOOKBACK: {LOOKBACK} | MIN_SEG: {MIN_SEG}\")\n",
    "print(f\"Train: {len(train_df)} (≤ {TRAIN_END.date()})\")\n",
    "print(f\"Val:   {len(val_df)} (≤ {VAL_END.date()})\")\n",
    "print(f\"Test:  {len(test_df)} (> {VAL_END.date()})\")\n",
    "\n",
    "assert len(train_df) >= MIN_SEG and len(val_df) >= MIN_SEG and len(test_df) >= MIN_SEG, \"Split mínimo não satisfeito.\"\n",
    "\n",
    "# ---------- 4) Escalonamento (fit só no train) ----------\n",
    "scaler = StandardScaler()\n",
    "X_train_tab = scaler.fit_transform(train_df[feature_cols].values)\n",
    "X_val_tab   = scaler.transform(val_df[feature_cols].values)\n",
    "X_test_tab  = scaler.transform(test_df[feature_cols].values)\n",
    "\n",
    "y_train = train_df[\"target\"].astype(int).values\n",
    "y_val   = val_df[\"target\"].astype(int).values\n",
    "y_test  = test_df[\"target\"].astype(int).values\n",
    "\n",
    "# ---------- 5) Construção de sequências ----------\n",
    "def make_sequences(X_tab, y_vec, lookback):\n",
    "    \"\"\"\n",
    "    Constrói janelas deslizantes alinhadas temporalmente.\n",
    "    Retorna (X_seq, y_seq) onde:\n",
    "      - X_seq: (n_samples, lookback, n_features)\n",
    "      - y_seq: (n_samples,)\n",
    "    \"\"\"\n",
    "    n = len(X_tab)\n",
    "    Xs, ys = [], []\n",
    "    for j in range(lookback-1, n):\n",
    "        Xs.append(X_tab[j - lookback + 1 : j + 1, :])\n",
    "        ys.append(y_vec[j])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train_tab, y_train, LOOKBACK)\n",
    "X_val_seq,   y_val_seq   = make_sequences(X_val_tab,   y_val,   LOOKBACK)\n",
    "X_test_seq,  y_test_seq  = make_sequences(X_test_tab,  y_test,  LOOKBACK)\n",
    "\n",
    "# ---------- 6) Persistências ----------\n",
    "with open(OUT_SPLIT, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"LOOKBACK\": LOOKBACK,\n",
    "        \"TRAIN_END\": TRAIN_END.strftime(\"%Y-%m-%d\"),\n",
    "        \"VAL_END\": VAL_END.strftime(\"%Y-%m-%d\"),\n",
    "        \"FEATURE_COLS\": feature_cols\n",
    "    }, f, indent=2)\n",
    "\n",
    "with open(OUT_SCALER, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "np.savez_compressed(\n",
    "    OUT_TAB,\n",
    "    X_train_tab=X_train_tab, y_train=y_train,\n",
    "    X_val_tab=X_val_tab,     y_val=y_val,\n",
    "    X_test_tab=X_test_tab,   y_test=y_test,\n",
    "    feature_cols=np.array(feature_cols, dtype=object)\n",
    ")\n",
    "\n",
    "np.savez_compressed(\n",
    "    OUT_SEQ,\n",
    "    X_train_seq=X_train_seq, y_train_seq=y_train_seq,\n",
    "    X_val_seq=X_val_seq,     y_val_seq=y_val_seq,\n",
    "    X_test_seq=X_test_seq,   y_test_seq=y_test_seq,\n",
    "    feature_cols=np.array(feature_cols, dtype=object),\n",
    "    lookback=np.array([LOOKBACK])\n",
    ")\n",
    "\n",
    "# ---------- 7) Relatório ----------\n",
    "def shape_str(arr):\n",
    "    return \"None\" if arr is None else str(arr.shape)\n",
    "\n",
    "print(\"\\n=== BLOCK 3B SUMMARY ===\")\n",
    "print(f\"FEATURES ({len(feature_cols)}): {feature_cols}\")\n",
    "print(\"\\nTABULAR:\")\n",
    "print(f\"X_train_tab: {X_train_tab.shape} | y_train: {y_train.shape}\")\n",
    "print(f\"X_val_tab:   {X_val_tab.shape}   | y_val:   {y_val.shape}\")\n",
    "print(f\"X_test_tab:  {X_test_tab.shape}  | y_test:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nSEQUENCES:\")\n",
    "print(f\"X_train_seq: {X_train_seq.shape} | y_train_seq: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq:   {X_val_seq.shape}   | y_val_seq:   {y_val_seq.shape}\")\n",
    "print(f\"X_test_seq:  {X_test_seq.shape}  | y_test_seq:  {y_test_seq.shape}\")\n",
    "\n",
    "print(f\"\\nSaved split boundaries → {OUT_SPLIT}\")\n",
    "print(f\"Saved scaler → {OUT_SCALER}\")\n",
    "print(f\"Saved TABULAR NPZ → {OUT_TAB}\")\n",
    "print(f\"Saved SEQUENCE NPZ → {OUT_SEQ}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562538c",
   "metadata": {},
   "source": [
    "# BLOCK 3C (v2): Suavização + Boruta com Tentative Rough Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from boruta import BorutaPy\n",
    "    HAS_BORUTA = True\n",
    "except Exception:\n",
    "    HAS_BORUTA = False\n",
    "\n",
    "DATA_DIR   = \"data\"\n",
    "IN_CLEAN   = os.path.join(DATA_DIR, \"merged_btc_features_CLEAN.csv\")\n",
    "SPLIT_JSON = os.path.join(DATA_DIR, \"split_boundaries.json\")\n",
    "\n",
    "OUT_SCALER      = os.path.join(DATA_DIR, \"scaler.pkl\")\n",
    "OUT_FEAT_TXT    = os.path.join(DATA_DIR, \"selected_features.txt\")\n",
    "OUT_ALLFEAT_TXT = os.path.join(DATA_DIR, \"feature_cols_order.txt\")\n",
    "OUT_SNAP_CSV    = os.path.join(DATA_DIR, \"model_features_snapshot_train.csv\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- Load clean data + split bounds ----------\n",
    "df = pd.read_csv(IN_CLEAN, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "with open(SPLIT_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "# ---------- Feature selection set ----------\n",
    "drop_cols = {\"date\", \"target\"}\n",
    "if \"price_usd\" in df.columns:\n",
    "    drop_cols.add(\"price_usd\")\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# ---------- Suavização (EMA 7 dias) ----------\n",
    "def ema_smooth(series, span=7):\n",
    "    return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "for c in feature_cols:\n",
    "    df[c] = ema_smooth(pd.to_numeric(df[c], errors=\"coerce\"))\n",
    "\n",
    "LOOKBACK  = int(meta[\"LOOKBACK\"])\n",
    "TRAIN_END = pd.Timestamp(meta[\"TRAIN_END\"])\n",
    "VAL_END   = pd.Timestamp(meta[\"VAL_END\"])\n",
    "\n",
    "train_df = df[df[\"date\"] <= TRAIN_END].copy()\n",
    "val_df   = df[(df[\"date\"] > TRAIN_END) & (df[\"date\"] <= VAL_END)].copy()\n",
    "test_df  = df[df[\"date\"] > VAL_END].copy()\n",
    "\n",
    "# ---------- Scale ----------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "X_val   = scaler.transform(val_df[feature_cols])\n",
    "X_test  = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "y_train = train_df[\"target\"].astype(int).values\n",
    "y_val   = val_df[\"target\"].astype(int).values\n",
    "y_test  = test_df[\"target\"].astype(int).values\n",
    "\n",
    "# ---------- Boruta with Tentative Rough Fix ----------\n",
    "def boruta_with_rough_fix(X, y, cols):\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=1000, n_jobs=-1,\n",
    "        class_weight=\"balanced\", random_state=SEED\n",
    "    )\n",
    "    boruta = BorutaPy(\n",
    "        estimator=rf, n_estimators=\"auto\",\n",
    "        verbose=1, random_state=SEED, max_iter=100\n",
    "    )\n",
    "    boruta.fit(X, y)\n",
    "    confirmed = np.array(cols)[boruta.support_]\n",
    "    tentative = np.array(cols)[boruta.support_weak_]\n",
    "    selected = list(set(confirmed) | set(tentative))\n",
    "    return selected, confirmed.tolist(), tentative.tolist()\n",
    "\n",
    "if HAS_BORUTA:\n",
    "    selected_cols, confirmed, tentative = boruta_with_rough_fix(X_train, y_train, feature_cols)\n",
    "    method_used = \"boruta_roughfix\"\n",
    "    if len(selected_cols) == 0:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=600, n_jobs=-1,\n",
    "            class_weight=\"balanced\", random_state=SEED\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "        order = np.argsort(rf.feature_importances_)[::-1]\n",
    "        selected_cols = [feature_cols[i] for i in order[:15]]\n",
    "        method_used = \"rf_fallback\"\n",
    "else:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=600, n_jobs=-1,\n",
    "        class_weight=\"balanced\", random_state=SEED\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    order = np.argsort(rf.feature_importances_)[::-1]\n",
    "    selected_cols = [feature_cols[i] for i in order[:15]]\n",
    "    method_used = \"rf_top15\"\n",
    "\n",
    "# ---------- Persist artifacts ----------\n",
    "joblib.dump(scaler, OUT_SCALER)\n",
    "with open(OUT_FEAT_TXT, \"w\") as f:\n",
    "    for c in selected_cols:\n",
    "        f.write(c + \"\\n\")\n",
    "with open(OUT_ALLFEAT_TXT, \"w\") as f:\n",
    "    for c in feature_cols:\n",
    "        f.write(c + \"\\n\")\n",
    "\n",
    "snap = pd.DataFrame(X_train, columns=feature_cols, index=train_df.index)\n",
    "snap.insert(0, \"date\", train_df[\"date\"].values)\n",
    "snap[selected_cols].to_csv(OUT_SNAP_CSV, index=False)\n",
    "\n",
    "print(\"\\n=== BLOCK 3C (v2) SUMMARY ===\")\n",
    "print(f\"Method: {method_used}\")\n",
    "print(f\"Selected ({len(selected_cols)}): {selected_cols}\")\n",
    "if method_used == \"boruta_roughfix\":\n",
    "    print(f\"Confirmed ({len(confirmed)}): {confirmed}\")\n",
    "    print(f\"Tentative ({len(tentative)}): {tentative}\")\n",
    "print(f\"Saved: {OUT_SCALER}, {OUT_FEAT_TXT}, {OUT_ALLFEAT_TXT}, {OUT_SNAP_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f14cf1",
   "metadata": {},
   "source": [
    "# BLOCK 3D: Reaplicar EMA(7), padronizar com scaler do 3C e materializar arrays com as FEATURES SELECIONADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = \"data\"\n",
    "IN_CLEAN   = os.path.join(DATA_DIR, \"merged_btc_features_CLEAN.csv\")\n",
    "SPLIT_JSON = os.path.join(DATA_DIR, \"split_boundaries.json\")\n",
    "SCALER_PKL = os.path.join(DATA_DIR, \"scaler.pkl\")\n",
    "SEL_TXT    = os.path.join(DATA_DIR, \"selected_features.txt\")\n",
    "\n",
    "# Saídas (somente features selecionadas)\n",
    "OUT_TAB_NPZ = os.path.join(DATA_DIR, \"tabular_train_val_test_SELECTED.npz\")\n",
    "OUT_SEQ_NPZ = os.path.join(DATA_DIR, \"sequence_train_val_test_SELECTED.npz\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def ema_smooth(df, cols, span=7):\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").ewm(span=span, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "def make_sequences(X, y, lookback):\n",
    "    \"\"\"\n",
    "    Janela [j-lookback+1 .. j] -> y[j]\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    seqs = []\n",
    "    targets = []\n",
    "    for j in range(lookback-1, n):\n",
    "        seqs.append(X[j - lookback + 1 : j + 1, :])\n",
    "        targets.append(y[j])\n",
    "    return np.array(seqs, dtype=np.float32), np.array(targets, dtype=np.int64)\n",
    "\n",
    "# ---------- Carregar dados e metadados ----------\n",
    "if not os.path.exists(IN_CLEAN):\n",
    "    raise FileNotFoundError(f\"Arquivo não encontrado: {IN_CLEAN}\")\n",
    "if not os.path.exists(SPLIT_JSON):\n",
    "    raise FileNotFoundError(f\"Split JSON não encontrado: {SPLIT_JSON}\")\n",
    "if not os.path.exists(SCALER_PKL):\n",
    "    raise FileNotFoundError(f\"Scaler não encontrado: {SCALER_PKL}\")\n",
    "if not os.path.exists(SEL_TXT):\n",
    "    raise FileNotFoundError(f\"Arquivo de features selecionadas não encontrado: {SEL_TXT}\")\n",
    "\n",
    "df = pd.read_csv(IN_CLEAN, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "with open(SPLIT_JSON, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "LOOKBACK  = int(meta[\"LOOKBACK\"])\n",
    "TRAIN_END = pd.Timestamp(meta[\"TRAIN_END\"])\n",
    "VAL_END   = pd.Timestamp(meta[\"VAL_END\"])\n",
    "\n",
    "with open(SEL_TXT, \"r\") as f:\n",
    "    selected_cols = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "\n",
    "# Definir colunas de features (vamos manter ‘price_usd’ fora, como antes)\n",
    "drop_cols = {\"date\", \"target\", \"price_usd\"}\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "# ---------- Suavização EMA(7) para consistência com o 3C ----------\n",
    "df = ema_smooth(df.copy(), feature_cols, span=7)\n",
    "\n",
    "# ---------- Reaplicar splits por data ----------\n",
    "train_df = df[df[\"date\"] <= TRAIN_END].copy()\n",
    "val_df   = df[(df[\"date\"] > TRAIN_END) & (df[\"date\"] <= VAL_END)].copy()\n",
    "test_df  = df[df[\"date\"] > VAL_END].copy()\n",
    "\n",
    "# Segurança: targets como int\n",
    "for part in (train_df, val_df, test_df):\n",
    "    if \"target\" not in part.columns:\n",
    "        raise RuntimeError(\"Coluna 'target' não encontrada. Rode o 3A antes do 3D.\")\n",
    "    part[\"target\"] = part[\"target\"].astype(int)\n",
    "\n",
    "# ---------- Carregar scaler treinado no 3C e transformar SOMENTE as selecionadas ----------\n",
    "scaler: StandardScaler = joblib.load(SCALER_PKL)\n",
    "\n",
    "# Observação importante:\n",
    "# O scaler foi ajustado no 3C com TODAS as features (suavizadas) na ordem de feature_cols.\n",
    "# Para transformar apenas as selecionadas mantendo parâmetros corretos,\n",
    "# criamos matrizes completas (todas as features), aplicamos scaler, e depois fatiamos pelas selecionadas.\n",
    "\n",
    "def transform_subset(df_part, all_cols, sel_cols, scaler):\n",
    "    X_all = df_part[all_cols].values\n",
    "    X_all_scaled = scaler.transform(X_all)   # mesma ordem do ajuste\n",
    "    # mapear índices das selecionadas dentro de all_cols\n",
    "    sel_idx = [all_cols.index(c) for c in sel_cols]\n",
    "    X_sel = X_all_scaled[:, sel_idx]\n",
    "    y = df_part[\"target\"].values\n",
    "    return X_sel, y\n",
    "\n",
    "X_train_tab, y_train = transform_subset(train_df, feature_cols, selected_cols, scaler)\n",
    "X_val_tab,   y_val   = transform_subset(val_df,   feature_cols, selected_cols, scaler)\n",
    "X_test_tab,  y_test  = transform_subset(test_df,  feature_cols, selected_cols, scaler)\n",
    "\n",
    "# ---------- Construir sequências (janelas) ----------\n",
    "X_train_seq, y_train_seq = make_sequences(X_train_tab, y_train, LOOKBACK)\n",
    "X_val_seq,   y_val_seq   = make_sequences(X_val_tab,   y_val,   LOOKBACK)\n",
    "X_test_seq,  y_test_seq  = make_sequences(X_test_tab,  y_test,  LOOKBACK)\n",
    "\n",
    "# ---------- Salvar NPZ selecionados ----------\n",
    "np.savez_compressed(\n",
    "    OUT_TAB_NPZ,\n",
    "    X_train=X_train_tab, y_train=y_train,\n",
    "    X_val=X_val_tab,     y_val=y_val,\n",
    "    X_test=X_test_tab,   y_test=y_test,\n",
    "    feature_names=np.array(selected_cols),\n",
    "    lookback=np.array([LOOKBACK])\n",
    ")\n",
    "np.savez_compressed(\n",
    "    OUT_SEQ_NPZ,\n",
    "    X_train=X_train_seq, y_train=y_train_seq,\n",
    "    X_val=X_val_seq,     y_val=y_val_seq,\n",
    "    X_test=X_test_seq,   y_test=y_test_seq,\n",
    "    feature_names=np.array(selected_cols),\n",
    "    lookback=np.array([LOOKBACK])\n",
    ")\n",
    "\n",
    "# ---------- Relatório ----------\n",
    "from collections import Counter\n",
    "print(\"=== BLOCK 3D (SELECTED) SUMMARY ===\")\n",
    "print(f\"LOOKBACK: {LOOKBACK}\")\n",
    "print(f\"Selected features ({len(selected_cols)}): {selected_cols}\")\n",
    "print(\"\\nTABULAR shapes:\")\n",
    "print(f\"  X_train_tab: {X_train_tab.shape} | y_train: {y_train.shape}\")\n",
    "print(f\"  X_val_tab:   {X_val_tab.shape}   | y_val:   {y_val.shape}\")\n",
    "print(f\"  X_test_tab:  {X_test_tab.shape}  | y_test:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nSEQUENCE shapes:\")\n",
    "print(f\"  X_train_seq: {X_train_seq.shape} | y_train_seq: {y_train_seq.shape}\")\n",
    "print(f\"  X_val_seq:   {X_val_seq.shape}   | y_val_seq:   {y_val_seq.shape}\")\n",
    "print(f\"  X_test_seq:  {X_test_seq.shape}  | y_test_seq:  {y_test_seq.shape}\")\n",
    "\n",
    "print(\"\\nClass balance (train/val/test):\")\n",
    "print(\"  train:\", Counter(y_train))\n",
    "print(\"  val  :\", Counter(y_val))\n",
    "print(\"  test :\", Counter(y_test))\n",
    "\n",
    "print(f\"\\nSaved TABULAR NPZ  → {OUT_TAB_NPZ}\")\n",
    "print(f\"Saved SEQUENCE NPZ → {OUT_SEQ_NPZ}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa662cdf",
   "metadata": {},
   "source": [
    "# BLOCK 4A: CNN–LSTM (sequências) com métricas completas e checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d659d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "NPZ_SEQ  = os.path.join(DATA_DIR, \"sequence_train_val_test_SELECTED.npz\")\n",
    "MODEL_F  = os.path.join(DATA_DIR, \"model_cnn_lstm_selected.keras\")\n",
    "HIST_CSV = os.path.join(DATA_DIR, \"history_cnn_lstm_selected.csv\")\n",
    "META_JSON= os.path.join(DATA_DIR, \"model_meta_selected.json\")\n",
    "\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ------------ 1) Carregar dados ------------\n",
    "if not os.path.exists(NPZ_SEQ):\n",
    "    raise FileNotFoundError(f\"NPZ não encontrado: {NPZ_SEQ}\")\n",
    "\n",
    "npz = np.load(NPZ_SEQ, allow_pickle=True)\n",
    "X_train = npz[\"X_train\"]; y_train = npz[\"y_train\"]\n",
    "X_val   = npz[\"X_val\"];   y_val   = npz[\"y_val\"]\n",
    "X_test  = npz[\"X_test\"];  y_test  = npz[\"y_test\"]\n",
    "feature_names = list(npz[\"feature_names\"])\n",
    "LOOKBACK = int(npz[\"lookback\"][0])\n",
    "\n",
    "n_train, T, D = X_train.shape\n",
    "print(f\"Shapes → train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "print(f\"LOOKBACK={LOOKBACK} | D (features)={D}\")\n",
    "print(\"Features:\", feature_names)\n",
    "\n",
    "# ------------ 2) Class weights (com base em y_train) ------------\n",
    "cnt = Counter(y_train.tolist())\n",
    "neg, pos = cnt.get(0, 0), cnt.get(1, 0)\n",
    "total = neg + pos\n",
    "# balanceamento: inverso da frequência\n",
    "w0 = total / (2.0 * max(neg, 1))\n",
    "w1 = total / (2.0 * max(pos, 1))\n",
    "class_weight = {0: w0, 1: w1}\n",
    "print(\"Class balance (train):\", cnt, \"| class_weight:\", class_weight)\n",
    "\n",
    "# ------------ 3) Montar o modelo CNN–LSTM ------------\n",
    "def build_cnn_lstm(input_shape, dropout=0.4):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, kernel_size=3, padding=\"causal\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LSTM(80)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"acc\"), keras.metrics.AUC(name=\"auc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_cnn_lstm((T, D), dropout=0.5)\n",
    "model.summary()\n",
    "\n",
    "# ------------ 4) Callbacks ------------\n",
    "ckpt = keras.callbacks.ModelCheckpoint(\n",
    "    MODEL_F, monitor=\"val_auc\", mode=\"max\", save_best_only=True, verbose=1\n",
    ")\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", mode=\"max\", patience=12, restore_best_weights=True, verbose=1\n",
    ")\n",
    "rlr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=6, min_lr=1e-5, verbose=1\n",
    ")\n",
    "csv = keras.callbacks.CSVLogger(HIST_CSV, append=False)\n",
    "\n",
    "# ------------ 5) Treinamento ------------\n",
    "BATCH = 64\n",
    "EPOCHS = 120\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[ckpt, es, rlr, csv],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------ 6) Recarregar melhor checkpoint (garantia) ------------\n",
    "best_model = keras.models.load_model(MODEL_F)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def find_best_threshold(y_true, prob):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)  # 0.10, 0.11, ..., 0.90\n",
    "    best_t, best_mcc = 0.5, -1.0\n",
    "    for t in thresholds:\n",
    "        pred = (prob >= t).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, pred)\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc, best_t = mcc, t\n",
    "    return best_t, best_mcc\n",
    "\n",
    "# 6) Recarregar melhor checkpoint (já está no teu código)\n",
    "best_model = keras.models.load_model(MODEL_F)\n",
    "\n",
    "# 7) Ajustar threshold com base no VAL\n",
    "prob_val = best_model.predict(X_val, verbose=0).ravel()\n",
    "t_best, mcc_best = find_best_threshold(y_val, prob_val)\n",
    "print(f\"Melhor threshold no VAL: {t_best:.3f} | MCC={mcc_best:.4f}\")\n",
    "\n",
    "# ------------ 7) Avaliação completa (val e test) ------------\n",
    "def evaluate_split(name, X, y, mdl, thr=0.5):\n",
    "    prob = mdl.predict(X, verbose=0).ravel()\n",
    "    print(f\"[{name}] prob stats → min={prob.min():.3f}, max={prob.max():.3f}, mean={prob.mean():.3f}\")\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y, pred)\n",
    "    pre = precision_score(y, pred, zero_division=0)\n",
    "    rec = recall_score(y, pred, zero_division=0)\n",
    "    f1  = f1_score(y, pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y, prob)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    mcc = matthews_corrcoef(y, pred)\n",
    "    cm  = confusion_matrix(y, pred)\n",
    "    print(f\"[{name}]  Acc={acc:.4f}  Prec={pre:.4f}  Rec={rec:.4f}  F1={f1:.4f}  AUC={auc:.4f}  MCC={mcc:.4f}\")\n",
    "    print(f\"[{name}]  Confusion matrix:\\n{cm}\")\n",
    "    return {\n",
    "        \"acc\":acc, \"prec\":pre, \"rec\":rec, \"f1\":f1, \"auc\":auc, \"mcc\":mcc,\n",
    "        \"cm\": cm.tolist()\n",
    "    }\n",
    "\n",
    "metrics_val  = evaluate_split(\"VAL\",  X_val,  y_val,  best_model, thr=t_best)\n",
    "metrics_test = evaluate_split(\"TEST\", X_test, y_test, best_model, thr=t_best)\n",
    "\n",
    "# ------------ 8) Salvar metadados ------------\n",
    "meta = {\n",
    "    \"lookback\": LOOKBACK,\n",
    "    \"features\": feature_names,\n",
    "    \"class_weight\": class_weight,\n",
    "    \"val\": metrics_val,\n",
    "    \"test\": metrics_test,\n",
    "    \"input_shapes\": {\n",
    "        \"train\": tuple(X_train.shape),\n",
    "        \"val\":   tuple(X_val.shape),\n",
    "        \"test\":  tuple(X_test.shape),\n",
    "    }\n",
    "}\n",
    "with open(META_JSON, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"\\n=== BLOCK 4A SUMMARY ===\")\n",
    "print(f\"Melhor modelo → {MODEL_F}\")\n",
    "print(f\"Histórico → {HIST_CSV}\")\n",
    "print(f\"Metadados → {META_JSON}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051675f4",
   "metadata": {},
   "source": [
    "# Backtest da estratégia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_F = os.path.join(DATA_DIR, \"model_cnn_lstm_selected.keras\")\n",
    "NPZ_SEQ = os.path.join(DATA_DIR, \"sequence_train_val_test_SELECTED.npz\")\n",
    "IN_CLEAN = os.path.join(DATA_DIR, \"merged_btc_features_CLEAN.csv\")\n",
    "POWELL_CSV = os.path.join(DATA_DIR, \"powell_classificacoes.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1) CARREGAR DADOS E MODELO\n",
    "# ============================================================================\n",
    "\n",
    "# Carregar modelo e dados\n",
    "best_model = keras.models.load_model(MODEL_F)\n",
    "\n",
    "# Carregar dados de teste com sequências\n",
    "npz = np.load(NPZ_SEQ, allow_pickle=True)\n",
    "X_test = npz[\"X_test\"]\n",
    "y_test = npz[\"y_test\"]\n",
    "feature_names = list(npz[\"feature_names\"])\n",
    "LOOKBACK = int(npz[\"lookback\"][0])\n",
    "\n",
    "# Carregar dados limpos originais (com datas)\n",
    "df_clean = pd.read_csv(IN_CLEAN, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Carregar dados Powell\n",
    "powell = pd.read_csv(POWELL_CSV, parse_dates=[\"date\"])\n",
    "\n",
    "# Carregar split boundaries\n",
    "with open(os.path.join(DATA_DIR, \"split_boundaries.json\"), \"r\") as f:\n",
    "    split_meta = json.load(f)\n",
    "TRAIN_END = pd.Timestamp(split_meta[\"TRAIN_END\"])\n",
    "VAL_END = pd.Timestamp(split_meta[\"VAL_END\"])\n",
    "\n",
    "# Extrair datas de teste\n",
    "test_df = df_clean[df_clean[\"date\"] > VAL_END].copy()\n",
    "test_df = test_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Ajustar para corresponder com sequências (primeiras LOOKBACK-1 linhas não têm sequência)\n",
    "test_dates = pd.to_datetime(test_df[\"date\"].values[LOOKBACK-1:])\n",
    "test_close = test_df[\"close\"].values[LOOKBACK-1:]\n",
    "\n",
    "print(f\"Test dates: {len(test_dates)} | Sequences: {len(X_test)} | Close prices: {len(test_close)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2) FAZER PREVISÕES\n",
    "# ============================================================================\n",
    "\n",
    "prob_test = best_model.predict(X_test, verbose=0).ravel()\n",
    "\n",
    "# Encontrar melhor threshold no VAL (já calculado no bloco anterior)\n",
    "X_val = npz[\"X_val\"]\n",
    "y_val = npz[\"y_val\"]\n",
    "val_df = df_clean[(df_clean[\"date\"] > TRAIN_END) & (df_clean[\"date\"] <= VAL_END)].copy()\n",
    "val_df = val_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "prob_val = best_model.predict(X_val, verbose=0).ravel()\n",
    "\n",
    "def find_best_threshold(y_true, prob):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_t, best_mcc = 0.5, -1.0\n",
    "    for t in thresholds:\n",
    "        pred = (prob >= t).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, pred)\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc, best_t = mcc, t\n",
    "    return best_t, best_mcc\n",
    "\n",
    "t_best, mcc_best = find_best_threshold(y_val, prob_val)\n",
    "print(f\"Threshold: {t_best:.3f} | MCC: {mcc_best:.4f}\")\n",
    "\n",
    "# Aplicar threshold\n",
    "pred_test = (prob_test >= t_best).astype(int)\n",
    "\n",
    "# ============================================================================\n",
    "# 3) PREPARAR SINAL POWELL (FFill entre pronunciamentos)\n",
    "# ============================================================================\n",
    "\n",
    "# Forward fill os pronunciamentos para cada dia\n",
    "powell_signal = pd.DataFrame({\n",
    "    \"date\": test_dates,\n",
    "    \"powell_class\": 0  # default neutro\n",
    "})\n",
    "\n",
    "powell = powell[[\"date\", \"classificacao\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "for idx, row in powell_signal.iterrows():\n",
    "    current_date = row[\"date\"]\n",
    "    # Pegar o pronunciamento mais recente anterior ou igual à data\n",
    "    recent = powell[powell[\"date\"] <= current_date].sort_values(\"date\", ascending=False)\n",
    "    if not recent.empty:\n",
    "        powell_signal.loc[idx, \"powell_class\"] = int(recent.iloc[0][\"classificacao\"])\n",
    "\n",
    "print(\"\\nPowellSignal distribuição:\")\n",
    "print(powell_signal[\"powell_class\"].value_counts().sort_index())\n",
    "\n",
    "# ============================================================================\n",
    "# 4) BACKTEST COM FILTRO POWELL\n",
    "# ============================================================================\n",
    "\n",
    "def run_backtest_with_powell_filter(dates, closes, predictions, powell_signal):\n",
    "    \"\"\"\n",
    "    Regras:\n",
    "    - Long quando pred=1 E powell_class=1\n",
    "    - Short quando pred=0 E powell_class=-1\n",
    "    - Sem posição quando powell_class=0\n",
    "    - Saída: sair de long quando pred muda para 0, saír de short quando pred muda para 1\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"date\": [],\n",
    "        \"close\": [],\n",
    "        \"pred\": [],\n",
    "        \"powell_class\": [],\n",
    "        \"position\": [],  # -1 short, 0 neutral, 1 long\n",
    "        \"entry_price\": [],\n",
    "        \"exit_price\": [],\n",
    "        \"pnl\": [],\n",
    "        \"return_pct\": []\n",
    "    }\n",
    "    \n",
    "    position = 0  # 0 = neutral, 1 = long, -1 = short\n",
    "    entry_price = 0\n",
    "    trades_log = []\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        date = dates[i]\n",
    "        close = closes[i]\n",
    "        pred = int(predictions[i])\n",
    "        powell = int(powell_signal.iloc[i][\"powell_class\"])\n",
    "        \n",
    "        pnl = 0\n",
    "        exit_price = 0\n",
    "        \n",
    "        # Lógica de entrada e saída\n",
    "        if position == 0:  # Sem posição aberta\n",
    "            # Verificar entrada em LONG\n",
    "            if pred == 1 and powell == 1:\n",
    "                position = 1\n",
    "                entry_price = close\n",
    "            # Verificar entrada em SHORT\n",
    "            elif pred == 0 and powell == -1:\n",
    "                position = -1\n",
    "                entry_price = close\n",
    "        \n",
    "        elif position == 1:  # Em LONG\n",
    "            # Sair do long quando pred muda para 0 (mesmo com powell=1)\n",
    "            if pred == 0:\n",
    "                exit_price = close\n",
    "                pnl = exit_price - entry_price\n",
    "                return_pct = (pnl / entry_price) * 100 if entry_price != 0 else 0\n",
    "                trades_log.append({\n",
    "                    \"type\": \"LONG\",\n",
    "                    \"entry_date\": results[\"date\"][-1] if results[\"date\"] else date,\n",
    "                    \"exit_date\": date,\n",
    "                    \"entry_price\": entry_price,\n",
    "                    \"exit_price\": exit_price,\n",
    "                    \"pnl\": pnl,\n",
    "                    \"return_pct\": return_pct\n",
    "                })\n",
    "                position = 0\n",
    "        \n",
    "        elif position == -1:  # Em SHORT\n",
    "            # Sair do short quando pred muda para 1 (mesmo com powell=-1)\n",
    "            if pred == 1:\n",
    "                exit_price = close\n",
    "                pnl = entry_price - exit_price  # short: lucro se close < entry\n",
    "                return_pct = (pnl / entry_price) * 100 if entry_price != 0 else 0\n",
    "                trades_log.append({\n",
    "                    \"type\": \"SHORT\",\n",
    "                    \"entry_date\": results[\"date\"][-1] if results[\"date\"] else date,\n",
    "                    \"exit_date\": date,\n",
    "                    \"entry_price\": entry_price,\n",
    "                    \"exit_price\": exit_price,\n",
    "                    \"pnl\": pnl,\n",
    "                    \"return_pct\": return_pct\n",
    "                })\n",
    "                position = 0\n",
    "        \n",
    "        results[\"date\"].append(date)\n",
    "        results[\"close\"].append(close)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"powell_class\"].append(powell)\n",
    "        results[\"position\"].append(position)\n",
    "        results[\"entry_price\"].append(entry_price if position != 0 else np.nan)\n",
    "        results[\"exit_price\"].append(exit_price if exit_price != 0 else np.nan)\n",
    "        results[\"pnl\"].append(pnl if exit_price != 0 else np.nan)\n",
    "        results[\"return_pct\"].append(return_pct if exit_price != 0 else np.nan)\n",
    "    \n",
    "    # Se ainda houver posição aberta no final, fechar no último preço\n",
    "    if position != 0:\n",
    "        close = closes[-1]\n",
    "        if position == 1:  # Long aberto\n",
    "            pnl = close - entry_price\n",
    "            return_pct = (pnl / entry_price) * 100\n",
    "            trades_log.append({\n",
    "                \"type\": \"LONG (UNCLOSED)\",\n",
    "                \"entry_date\": results[\"date\"][-2] if len(results[\"date\"]) > 1 else dates[-1],\n",
    "                \"exit_date\": dates[-1],\n",
    "                \"entry_price\": entry_price,\n",
    "                \"exit_price\": close,\n",
    "                \"pnl\": pnl,\n",
    "                \"return_pct\": return_pct\n",
    "            })\n",
    "        elif position == -1:  # Short aberto\n",
    "            pnl = entry_price - close\n",
    "            return_pct = (pnl / entry_price) * 100\n",
    "            trades_log.append({\n",
    "                \"type\": \"SHORT (UNCLOSED)\",\n",
    "                \"entry_date\": results[\"date\"][-2] if len(results[\"date\"]) > 1 else dates[-1],\n",
    "                \"exit_date\": dates[-1],\n",
    "                \"entry_price\": entry_price,\n",
    "                \"exit_price\": close,\n",
    "                \"pnl\": pnl,\n",
    "                \"return_pct\": return_pct\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(trades_log)\n",
    "\n",
    "# Executar backtest\n",
    "backtest_df, trades_df = run_backtest_with_powell_filter(\n",
    "    test_dates, test_close, pred_test, powell_signal\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BACKTEST RESUMO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPeríodo: {pd.Timestamp(test_dates[0]).date()} a {pd.Timestamp(test_dates[-1]).date()}\")\n",
    "print(f\"Total de dias: {len(test_dates)}\")\n",
    "print(f\"\\nDistribuição de sinais (pred):\")\n",
    "print(f\"  Long (1): {np.sum(pred_test == 1)}\")\n",
    "print(f\"  Short (0): {np.sum(pred_test == 0)}\")\n",
    "\n",
    "print(f\"\\nDistribuição Powell:\")\n",
    "print(f\"  Positivo (1): {np.sum(powell_signal['powell_class'] == 1)}\")\n",
    "print(f\"  Neutro (0): {np.sum(powell_signal['powell_class'] == 0)}\")\n",
    "print(f\"  Negativo (-1): {np.sum(powell_signal['powell_class'] == -1)}\")\n",
    "\n",
    "print(f\"\\n--- TRADES EXECUTADOS ---\")\n",
    "print(f\"Total de trades: {len(trades_df)}\")\n",
    "\n",
    "if len(trades_df) > 0:\n",
    "    print(f\"\\nDetalhamento de trades:\")\n",
    "    for idx, trade in trades_df.iterrows():\n",
    "        print(f\"\\nTrade {idx+1}:\")\n",
    "        print(f\"  Tipo: {trade['type']}\")\n",
    "        print(f\"  Entry: {pd.Timestamp(trade['entry_date']).date()} @ ${trade['entry_price']:.2f}\")\n",
    "        print(f\"  Exit:  {pd.Timestamp(trade['exit_date']).date()} @ ${trade['exit_price']:.2f}\")\n",
    "        print(f\"  P&L: ${trade['pnl']:.2f} ({trade['return_pct']:+.2f}%)\")\n",
    "    \n",
    "    # Sumário de performance\n",
    "    total_pnl = trades_df[\"pnl\"].sum()\n",
    "    avg_return = trades_df[\"return_pct\"].mean()\n",
    "    winning_trades = len(trades_df[trades_df[\"pnl\"] > 0])\n",
    "    losing_trades = len(trades_df[trades_df[\"pnl\"] < 0])\n",
    "    win_rate = (winning_trades / len(trades_df) * 100) if len(trades_df) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- PERFORMANCE SUMMARY ---\")\n",
    "    print(f\"Total P&L: ${total_pnl:+.2f}\")\n",
    "    print(f\"Avg Return per Trade: {avg_return:+.2f}%\")\n",
    "    print(f\"Win Rate: {win_rate:.1f}% ({winning_trades}W / {losing_trades}L)\")\n",
    "    print(f\"Largest Win: ${trades_df['pnl'].max():.2f} ({trades_df['return_pct'].max():+.2f}%)\")\n",
    "    print(f\"Largest Loss: ${trades_df['pnl'].min():.2f} ({trades_df[trades_df['pnl'] < 0]['return_pct'].min():+.2f}%)\")\n",
    "    \n",
    "    # Calcular profit factor (gross profit / gross loss)\n",
    "    gross_profit = trades_df[trades_df[\"pnl\"] > 0][\"pnl\"].sum()\n",
    "    gross_loss = abs(trades_df[trades_df[\"pnl\"] < 0][\"pnl\"].sum())\n",
    "    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
    "    print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "else:\n",
    "    print(\"Nenhum trade executado no período!\")\n",
    "\n",
    "# Mostrar histórico de posições\n",
    "print(\"\\n--- ÚLTIMAS 20 LINHAS DO BACKTEST ---\")\n",
    "print(backtest_df[[\"date\", \"close\", \"pred\", \"powell_class\", \"position\"]].tail(20).to_string())\n",
    "\n",
    "# Salvar resultados\n",
    "backtest_df.to_csv(os.path.join(DATA_DIR, \"backtest_results.csv\"), index=False)\n",
    "trades_df.to_csv(os.path.join(DATA_DIR, \"backtest_trades.csv\"), index=False)\n",
    "print(f\"\\n✓ Resultados salvos em {os.path.join(DATA_DIR, 'backtest_results.csv')}\")\n",
    "print(f\"✓ Trades salvos em {os.path.join(DATA_DIR, 'backtest_trades.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ac669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização de Resultados do Backtest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# ============================================================================\n",
    "# CARREGAR RESULTADOS\n",
    "# ============================================================================\n",
    "\n",
    "backtest_results = pd.read_csv(os.path.join(DATA_DIR, \"backtest_results.csv\"), parse_dates=[\"date\"])\n",
    "trades_results = pd.read_csv(os.path.join(DATA_DIR, \"backtest_trades.csv\"), parse_dates=[\"entry_date\", \"exit_date\"])\n",
    "\n",
    "print(\"Resultados carregados:\")\n",
    "print(f\"  Dias analisados: {len(backtest_results)}\")\n",
    "print(f\"  Trades executados: {len(trades_results)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GRÁFICO 1: Preço BTC + Posições + Sinais Powell\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "# Eixo 1: Preço com positions\n",
    "ax1 = axes[0]\n",
    "dates = backtest_results[\"date\"].values\n",
    "closes = backtest_results[\"close\"].values\n",
    "\n",
    "ax1.plot(dates, closes, label=\"BTC Close\", color=\"black\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "x = backtest_results[\"date\"].values\n",
    "y1 = closes * 0.98\n",
    "y2 = closes * 1.02\n",
    "\n",
    "long_mask  = backtest_results[\"position\"] == 1\n",
    "short_mask = backtest_results[\"position\"] == -1\n",
    "flat_mask  = backtest_results[\"position\"] == 0\n",
    "\n",
    "ax1.fill_between(x, y1, y2, where=long_mask,  color=\"green\", alpha=0.3, label=\"Long Position\")\n",
    "ax1.fill_between(x, y1, y2, where=short_mask, color=\"red\",   alpha=0.3, label=\"Short Position\")\n",
    "ax1.fill_between(x, y1, y2, where=flat_mask,  color=\"white\", alpha=0.3, label=\"No Position\")\n",
    "\n",
    "# Marcar trades\n",
    "for idx, trade in trades_results.iterrows():\n",
    "    entry_price = trade[\"entry_price\"]\n",
    "    exit_price = trade[\"exit_price\"]\n",
    "    pnl = trade[\"pnl\"]\n",
    "    \n",
    "    if trade[\"type\"].startswith(\"LONG\"):\n",
    "        color = \"green\" if pnl > 0 else \"darkgreen\"\n",
    "    else:\n",
    "        color = \"red\" if pnl > 0 else \"darkred\"\n",
    "    \n",
    "    ax1.scatter(trade[\"entry_date\"], entry_price, color=color, s=100, marker=\"o\", zorder=5)\n",
    "    ax1.scatter(trade[\"exit_date\"], exit_price, color=color, s=100, marker=\"x\", zorder=5)\n",
    "\n",
    "ax1.set_ylabel(\"BTC Price ($)\", fontsize=11, fontweight=\"bold\")\n",
    "ax1.set_title(\"BTC Price + Strategy Positions\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Eixo 2: Sinais de Previsão\n",
    "ax2 = axes[1]\n",
    "colors_pred = [\"red\" if p == 0 else \"green\" for p in backtest_results[\"pred\"]]\n",
    "ax2.scatter(dates, backtest_results[\"pred\"], c=colors_pred, s=30, alpha=0.6, label=\"Model Prediction\")\n",
    "ax2.set_ylabel(\"Signal (0=Short, 1=Long)\", fontsize=11, fontweight=\"bold\")\n",
    "ax2.set_title(\"Model Predictions\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "ax2.legend()\n",
    "\n",
    "# Eixo 3: Sinal Powell (Forward Filled)\n",
    "ax3 = axes[2]\n",
    "colors_powell = []\n",
    "for p in backtest_results[\"powell_class\"]:\n",
    "    if p == 1:\n",
    "        colors_powell.append(\"green\")\n",
    "    elif p == -1:\n",
    "        colors_powell.append(\"red\")\n",
    "    else:\n",
    "        colors_powell.append(\"gray\")\n",
    "\n",
    "ax3.scatter(dates, backtest_results[\"powell_class\"], c=colors_powell, s=30, alpha=0.6, label=\"Powell Signal\")\n",
    "ax3.set_ylabel(\"Powell Class (-1/0/1)\", fontsize=11, fontweight=\"bold\")\n",
    "ax3.set_xlabel(\"Date\", fontsize=11, fontweight=\"bold\")\n",
    "ax3.set_title(\"Powell Sentiment Signal (Forward Filled)\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_ylim(-1.5, 1.5)\n",
    "ax3.grid(True, alpha=0.3, axis=\"y\")\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, \"backtest_analysis.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\n✓ Gráfico salvo em {os.path.join(DATA_DIR, 'backtest_analysis.png')}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# GRÁFICO 2: Equity Curve (Curva de Lucro Acumulado)\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Calcular equity curve baseado em trades\n",
    "equity = [0]\n",
    "trade_dates = [backtest_results[\"date\"].iloc[0]]\n",
    "\n",
    "for idx, trade in trades_results.iterrows():\n",
    "    equity.append(equity[-1] + trade[\"pnl\"])\n",
    "    trade_dates.append(trade[\"exit_date\"])\n",
    "\n",
    "# Plotar equity curve\n",
    "ax.plot(trade_dates, equity, marker=\"o\", linewidth=2, markersize=8, color=\"navy\", label=\"Cumulative P&L\")\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "\n",
    "# Colorir fundo baseado em ganhos/perdas\n",
    "for i in range(len(equity)-1):\n",
    "    color = \"lightgreen\" if equity[i+1] > equity[i] else \"lightcoral\"\n",
    "    ax.fill_between([trade_dates[i], trade_dates[i+1]], equity[i], equity[i+1], \n",
    "                    alpha=0.3, color=color)\n",
    "\n",
    "ax.set_xlabel(\"Date\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Cumulative P&L ($)\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_title(\"Strategy Equity Curve\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, \"equity_curve.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"✓ Equity Curve salvo em {os.path.join(DATA_DIR, 'equity_curve.png')}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# GRÁFICO 3: Distribuição de Returns\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "returns = trades_results[\"return_pct\"].values\n",
    "colors_return = [\"green\" if r > 0 else \"red\" for r in returns]\n",
    "\n",
    "bars = ax.bar(range(len(returns)), returns, color=colors_return, alpha=0.7, edgecolor=\"black\")\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "ax.set_xlabel(\"Trade #\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Return (%)\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_title(\"Return per Trade\", fontsize=12, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, \"returns_distribution.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"✓ Returns Distribution salvo em {os.path.join(DATA_DIR, 'returns_distribution.png')}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMÁRIO ESTATÍSTICO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO ESTATÍSTICO DO BACKTEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(trades_results) > 0:\n",
    "    # Métricas básicas\n",
    "    total_trades = len(trades_results)\n",
    "    total_pnl = trades_results[\"pnl\"].sum()\n",
    "    total_return_pct = (total_pnl / backtest_results[\"close\"].iloc[0] * 100) if backtest_results[\"close\"].iloc[0] > 0 else 0\n",
    "    \n",
    "    # Winning and losing trades\n",
    "    winning = trades_results[trades_results[\"pnl\"] > 0]\n",
    "    losing = trades_results[trades_results[\"pnl\"] < 0]\n",
    "    n_wins = len(winning)\n",
    "    n_losses = len(losing)\n",
    "    win_rate = (n_wins / total_trades * 100) if total_trades > 0 else 0\n",
    "    \n",
    "    # P&L statistics\n",
    "    avg_win = winning[\"pnl\"].mean() if len(winning) > 0 else 0\n",
    "    avg_loss = losing[\"pnl\"].mean() if len(losing) > 0 else 0\n",
    "    max_win = winning[\"pnl\"].max() if len(winning) > 0 else 0\n",
    "    max_loss = losing[\"pnl\"].min() if len(losing) > 0 else 0\n",
    "    \n",
    "    # Return statistics\n",
    "    avg_return_win = winning[\"return_pct\"].mean() if len(winning) > 0 else 0\n",
    "    avg_return_loss = losing[\"return_pct\"].mean() if len(losing) > 0 else 0\n",
    "    \n",
    "    # Profit factor\n",
    "    gross_profit = winning[\"pnl\"].sum() if len(winning) > 0 else 0\n",
    "    gross_loss = abs(losing[\"pnl\"].sum()) if len(losing) > 0 else 0\n",
    "    profit_factor = gross_profit / gross_loss if gross_loss > 0 else (float('inf') if gross_profit > 0 else 0)\n",
    "    \n",
    "    print(f\"\\nTotais:\")\n",
    "    print(f\"  Total de Trades: {total_trades}\")\n",
    "    print(f\"  Total P&L: ${total_pnl:+.2f}\")\n",
    "    print(f\"  Total Return: {total_return_pct:+.2f}%\")\n",
    "    \n",
    "    print(f\"\\nWin/Loss:\")\n",
    "    print(f\"  Trades Vencedores: {n_wins} ({win_rate:.1f}%)\")\n",
    "    print(f\"  Trades Perdedores: {n_losses}\")\n",
    "    \n",
    "    print(f\"\\nP&L por Trade:\")\n",
    "    print(f\"  Avg Win: ${avg_win:+.2f} ({avg_return_win:+.2f}%)\")\n",
    "    print(f\"  Avg Loss: ${avg_loss:+.2f} ({avg_return_loss:+.2f}%)\")\n",
    "    print(f\"  Max Win: ${max_win:+.2f}\")\n",
    "    print(f\"  Max Loss: ${max_loss:+.2f}\")\n",
    "    \n",
    "    print(f\"\\nRazões:\")\n",
    "    print(f\"  Profit Factor: {profit_factor:.2f}\" if profit_factor != float('inf') else f\"  Profit Factor: ∞\")\n",
    "    if len(winning) > 0 and len(losing) > 0:\n",
    "        print(f\"  Avg Win / Avg Loss: {abs(avg_win / avg_loss):.2f}\" if avg_loss != 0 else \"  Avg Win / Avg Loss: ∞\")\n",
    "    \n",
    "    print(f\"\\nDuração dos Trades:\")\n",
    "    trades_results[\"duration\"] = (trades_results[\"exit_date\"] - trades_results[\"entry_date\"]).dt.days\n",
    "    print(f\"  Média: {trades_results['duration'].mean():.1f} dias\")\n",
    "    print(f\"  Min: {trades_results['duration'].min()} dias\")\n",
    "    print(f\"  Max: {trades_results['duration'].max()} dias\")\n",
    "else:\n",
    "    print(\"Nenhum trade foi executado!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c90b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
